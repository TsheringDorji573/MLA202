{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae9c9ad",
   "metadata": {},
   "source": [
    "# MLA202: Reinforcement Learning - Take-Home Practical\n",
    "\n",
    "**Student ID:** 03240424\n",
    "**Student Name:** Tshering dorji \n",
    "\n",
    "This notebook contains solutions for **Problem 1, Problem 2, and Problem 3**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a174e41",
   "metadata": {},
   "source": [
    "## Problem 1: Q-Learning on GridWorld (3 marks)\n",
    "- Implement GridWorld environment\n",
    "- Implement Q-learning algorithm\n",
    "- Train agent and visualize rewards\n",
    "- Show learned policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0eaf026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Derived Policy:\n",
      "----------------------------\n",
      "S  →  ↓  →  ↓\n",
      "↓  #  ↓  #  ↓\n",
      "→  →  ↓  #  ↓\n",
      "↓  #  →  →  ↓\n",
      "→  →  →  →  G\n",
      "----------------------------\n",
      "Reward curve saved as gridworld_rewards.png\n",
      "Success: 20/20\n",
      "Mean steps to goal: 8.0\n",
      "\n",
      "Q-table Summary:\n",
      "Shape: (21, 4)\n",
      "Non-zero entries: 80\n",
      "Largest Q value: 9.999999999999995\n",
      "Smallest Q value: -0.5261204578470817\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Environment Definition\n",
    "# ------------------------------------------------------------\n",
    "class GridWorldEnv:\n",
    "    \"\"\"\n",
    "    A compact 5×5 navigation environment.\n",
    "\n",
    "    Layout:\n",
    "        S . . . .\n",
    "        . # . # .\n",
    "        . . . # .\n",
    "        . # . . .\n",
    "        . . . . G\n",
    "\n",
    "    S = (0,0)\n",
    "    G = (4,4)\n",
    "    Walls block movement.\n",
    "    \"\"\"\n",
    "\n",
    "    ACTIONS = {\n",
    "        0: (-1, 0),   # up\n",
    "        1: (1, 0),    # down\n",
    "        2: (0, -1),   # left\n",
    "        3: (0, 1)     # right\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.size = 5\n",
    "        self.start_pos = (0, 0)\n",
    "        self.goal_pos = (4, 4)\n",
    "\n",
    "        self.blocked = {(1,1), (1,3), (2,3), (3,1)}\n",
    "\n",
    "        # Precompute valid state coordinates\n",
    "        coords = []\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                if (r, c) not in self.blocked:\n",
    "                    coords.append((r, c))\n",
    "        self.valid_cells = coords\n",
    "        self.state_count = len(coords)\n",
    "\n",
    "        self.current = self.start_pos\n",
    "\n",
    "    # ---------------------------\n",
    "    # Core environment operations\n",
    "    # ---------------------------\n",
    "    def reset(self):\n",
    "        self.current = self.start_pos\n",
    "        return self._to_index(self.current)\n",
    "\n",
    "    def step(self, action):\n",
    "        dr, dc = self.ACTIONS[action]\n",
    "        nr, nc = self.current[0] + dr, self.current[1] + dc\n",
    "\n",
    "        # Out of map or wall\n",
    "        if (\n",
    "            nr < 0 or nr >= self.size or\n",
    "            nc < 0 or nc >= self.size or\n",
    "            (nr, nc) in self.blocked\n",
    "        ):\n",
    "            # Invalid move → penalty, remain in same location\n",
    "            reward = -1\n",
    "            new_cell = self.current\n",
    "        else:\n",
    "            new_cell = (nr, nc)\n",
    "\n",
    "            if new_cell == self.goal_pos:\n",
    "                reward = 10\n",
    "                self.current = new_cell\n",
    "                return self._to_index(new_cell), reward, True\n",
    "            else:\n",
    "                reward = -0.1\n",
    "\n",
    "        self.current = new_cell\n",
    "        terminal = self.current == self.goal_pos\n",
    "        return self._to_index(new_cell), reward, terminal\n",
    "\n",
    "    # ---------------------------\n",
    "    # State/index conversion\n",
    "    # ---------------------------\n",
    "    def _to_index(self, cell):\n",
    "        try:\n",
    "            return self.valid_cells.index(cell)\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "    def _from_index(self, idx):\n",
    "        if 0 <= idx < self.state_count:\n",
    "            return self.valid_cells[idx]\n",
    "        return None\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Q-Learning Agent\n",
    "# ------------------------------------------------------------\n",
    "def run_q_learning(env, episodes=1000, lr=0.15, disc=0.98, eps=0.12):\n",
    "    \"\"\"\n",
    "    Perform Q-learning on the supplied GridWorld.\n",
    "\n",
    "    Returns:\n",
    "        Q-table, rewards list\n",
    "    \"\"\"\n",
    "\n",
    "    num_actions = 4\n",
    "    Q = np.zeros((env.state_count, num_actions))\n",
    "    episode_returns = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        for _ in range(120):  # execution cap\n",
    "            # ε-greedy behaviour\n",
    "            if np.random.rand() < eps:\n",
    "                a = np.random.randint(num_actions)\n",
    "            else:\n",
    "                a = np.argmax(Q[s])\n",
    "\n",
    "            ns, r, done = env.step(a)\n",
    "\n",
    "            # compute target\n",
    "            best_next = np.max(Q[ns])\n",
    "            target = r + disc * best_next\n",
    "\n",
    "            # incremental update\n",
    "            Q[s, a] += lr * (target - Q[s, a])\n",
    "\n",
    "            ep_reward += r\n",
    "            s = ns\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_returns.append(ep_reward)\n",
    "\n",
    "    return Q, episode_returns\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Visualization Utilities\n",
    "# ------------------------------------------------------------\n",
    "def display_policy(env, Q):\n",
    "    arrows = {0:'↑', 1:'↓', 2:'←', 3:'→'}\n",
    "    print(\"\\nDerived Policy:\")\n",
    "    print(\"-\" * 28)\n",
    "\n",
    "    for r in range(env.size):\n",
    "        row_symbols = []\n",
    "        for c in range(env.size):\n",
    "            if (r, c) == env.start_pos:\n",
    "                row_symbols.append(\"S\")\n",
    "            elif (r, c) == env.goal_pos:\n",
    "                row_symbols.append(\"G\")\n",
    "            elif (r, c) in env.blocked:\n",
    "                row_symbols.append(\"#\")\n",
    "            else:\n",
    "                idx = env._to_index((r, c))\n",
    "                action = np.argmax(Q[idx])\n",
    "                row_symbols.append(arrows[action])\n",
    "        print(\"  \".join(row_symbols))\n",
    "    print(\"-\" * 28)\n",
    "\n",
    "\n",
    "def plot_rewards(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.plot(history, alpha=0.55)\n",
    "    plt.title(\"Episode Reward Progression\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"gridworld_rewards.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Reward curve saved as gridworld_rewards.png\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Agent Evaluation\n",
    "# ------------------------------------------------------------\n",
    "def evaluate(env, Q, trials=10):\n",
    "    successes = 0\n",
    "    steps_list = []\n",
    "\n",
    "    for _ in range(trials):\n",
    "        s = env.reset()\n",
    "        counter = 0\n",
    "\n",
    "        for _ in range(120):\n",
    "            a = np.argmax(Q[s])\n",
    "            s, r, done = env.step(a)\n",
    "            counter += 1\n",
    "\n",
    "            if done:\n",
    "                successes += 1\n",
    "                steps_list.append(counter)\n",
    "                break\n",
    "\n",
    "    print(f\"Success: {successes}/{trials}\")\n",
    "    if steps_list:\n",
    "        print(f\"Mean steps to goal: {np.mean(steps_list):.1f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Driver\n",
    "# ------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    env = GridWorldEnv()\n",
    "\n",
    "    Q, logs = run_q_learning(\n",
    "        env,\n",
    "        episodes=1000,\n",
    "        lr=0.15,\n",
    "        disc=0.98,\n",
    "        eps=0.15\n",
    "    )\n",
    "\n",
    "    display_policy(env, Q)\n",
    "    plot_rewards(logs)\n",
    "    evaluate(env, Q, trials=20)\n",
    "\n",
    "    print(\"\\nQ-table Summary:\")\n",
    "    print(\"Shape:\", Q.shape)\n",
    "    print(\"Non-zero entries:\", np.count_nonzero(Q))\n",
    "    print(\"Largest Q value:\", np.max(Q))\n",
    "    print(\"Smallest Q value:\", np.min(Q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f3d59",
   "metadata": {},
   "source": [
    "## Problem 2: Deep Q-Network (DQN) Completion (4 marks)\n",
    "\n",
    "**Task:** Complete the DQN implementation to solve the CartPole-v1 environment.\n",
    "\n",
    "- Define a neural network with 2 hidden layers of size 128 and ReLU activations.\n",
    "- Implement a replay buffer for experience replay.\n",
    "- Implement a DQN agent with epsilon-greedy action selection.\n",
    "- Train the agent for 500 episodes.\n",
    "- Plot rewards over episodes to show learning progress.\n",
    "- Achieve an average reward > 400 over the last 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a4d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Deep Q-Network on CartPole-v1\n",
      "============================================================\n",
      "Episode 50/500\n",
      "  Latest Reward: 203.0\n",
      "  100-episode Avg: 92.88\n",
      "  Epsilon: 0.010\n",
      "----------------------------------------\n",
      "Episode 100/500\n",
      "  Latest Reward: 128.0\n",
      "  100-episode Avg: 152.42\n",
      "  Epsilon: 0.010\n",
      "----------------------------------------\n",
      "Episode 150/500\n",
      "  Latest Reward: 178.0\n",
      "  100-episode Avg: 197.37\n",
      "  Epsilon: 0.010\n",
      "----------------------------------------\n",
      "Episode 200/500\n",
      "  Latest Reward: 19.0\n",
      "  100-episode Avg: 206.48\n",
      "  Epsilon: 0.010\n",
      "----------------------------------------\n",
      "Episode 250/500\n",
      "  Latest Reward: 375.0\n",
      "  100-episode Avg: 270.90\n",
      "  Epsilon: 0.010\n",
      "----------------------------------------\n",
      "Episode 300/500\n",
      "  Latest Reward: 281.0\n",
      "  100-episode Avg: 291.11\n",
      "  Epsilon: 0.010\n",
      "----------------------------------------\n",
      "Episode 350/500\n",
      "  Latest Reward: 20.0\n",
      "  100-episode Avg: 196.36\n",
      "  Epsilon: 0.010\n",
      "----------------------------------------\n",
      "Episode 400/500\n",
      "  Latest Reward: 136.0\n",
      "  100-episode Avg: 122.68\n",
      "  Epsilon: 0.010\n",
      "----------------------------------------\n",
      "Episode 450/500\n",
      "  Latest Reward: 337.0\n",
      "  100-episode Avg: 173.68\n",
      "  Epsilon: 0.010\n",
      "----------------------------------------\n",
      "Episode 500/500\n",
      "  Latest Reward: 160.0\n",
      "  100-episode Avg: 233.46\n",
      "  Epsilon: 0.010\n",
      "----------------------------------------\n",
      "Plot saved as dqn_cartpole_results.png\n",
      "\n",
      "Evaluating trained agent...\n",
      "Episode 1: Reward = 119.0\n",
      "Episode 2: Reward = 126.0\n",
      "Episode 3: Reward = 125.0\n",
      "Episode 4: Reward = 118.0\n",
      "Episode 5: Reward = 120.0\n",
      "Episode 6: Reward = 129.0\n",
      "Episode 7: Reward = 123.0\n",
      "Episode 8: Reward = 127.0\n",
      "Episode 9: Reward = 129.0\n",
      "Episode 10: Reward = 121.0\n",
      "\n",
      "Evaluation Summary:\n",
      "  Average Score: 123.70\n",
      "  Std Dev: 3.87\n",
      "  Min/Max: 118.0 / 129.0\n",
      "\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Neural Network Model for Q-value Approximation\n",
    "# --------------------------------------------------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Feed-forward neural network used to approximate Q(s,a).\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Experience Replay Buffer\n",
    "# --------------------------------------------------------------------\n",
    "class Memory:\n",
    "    \"\"\"Stores experience tuples for sampling during training.\"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.storage = deque(maxlen=size)\n",
    "\n",
    "    def add(self, s, a, r, s_next, done):\n",
    "        self.storage.append((s, a, r, s_next, done))\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = random.sample(self.storage, n)\n",
    "        s, a, r, s_next, d = zip(*batch)\n",
    "\n",
    "        return (\n",
    "            np.array(s, dtype=np.float32),\n",
    "            np.array(a, dtype=np.int64),\n",
    "            np.array(r, dtype=np.float32),\n",
    "            np.array(s_next, dtype=np.float32),\n",
    "            np.array(d, dtype=np.float32)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.storage)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# DQN Agent\n",
    "# --------------------------------------------------------------------\n",
    "class DeepQAgent:\n",
    "    \"\"\"Encapsulates policy selection, learning, and target network updates.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.n_actions = action_dim\n",
    "\n",
    "        # Networks\n",
    "        self.policy = QNetwork(state_dim, action_dim)\n",
    "        self.target = QNetwork(state_dim, action_dim)\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_lower = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # Replay buffer\n",
    "        self.memory = Memory(size=10000)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-3)\n",
    "\n",
    "    def choose_action(self, state, explore=True):\n",
    "        \"\"\"Epsilon-greedy policy.\"\"\"\n",
    "        if explore and np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_vals = self.policy(state)\n",
    "        return q_vals.argmax().item()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Performs a gradient update if enough samples are available.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        s, a, r, s_next, d = self.memory.sample(self.batch_size)\n",
    "\n",
    "        s = torch.tensor(s)\n",
    "        a = torch.tensor(a).unsqueeze(1)\n",
    "        r = torch.tensor(r)\n",
    "        s_next = torch.tensor(s_next)\n",
    "        d = torch.tensor(d)\n",
    "\n",
    "        # Current Q-values for actions taken\n",
    "        q_pred = self.policy(s).gather(1, a).squeeze()\n",
    "\n",
    "        # Target Q-values using target network\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target(s_next).max(1)[0]\n",
    "            q_target = r + (1 - d) * self.gamma * next_q\n",
    "\n",
    "        loss = nn.MSELoss()(q_pred, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Epsilon decay\n",
    "        if self.epsilon > self.epsilon_lower:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_target(self):\n",
    "        \"\"\"Update target network parameters.\"\"\"\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Training Routine\n",
    "# --------------------------------------------------------------------\n",
    "def train_cartpole(num_episodes=500):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    agent = DeepQAgent(state_dim, action_dim)\n",
    "\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training Deep Q-Network on CartPole-v1\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "\n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "\n",
    "        total_reward = 0\n",
    "        ep_losses = []\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state, explore=True)\n",
    "            step_result = env.step(action)\n",
    "\n",
    "            # Support both new and old Gym APIs\n",
    "            if len(step_result) == 5:\n",
    "                next_state, reward, terminated, truncated, _ = step_result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = step_result\n",
    "\n",
    "            agent.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "            loss = agent.learn()\n",
    "            if loss is not None:\n",
    "                ep_losses.append(loss)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Update target network periodically\n",
    "        if (ep + 1) % 10 == 0:\n",
    "            agent.sync_target()\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        if ep_losses:\n",
    "            losses.append(np.mean(ep_losses))\n",
    "\n",
    "        # Status update\n",
    "        if (ep + 1) % 50 == 0:\n",
    "            avg100 = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            print(f\"Episode {ep+1}/{num_episodes}\")\n",
    "            print(f\"  Latest Reward: {total_reward}\")\n",
    "            print(f\"  100-episode Avg: {avg100:.2f}\")\n",
    "            print(f\"  Epsilon: {agent.epsilon:.3f}\")\n",
    "            print(\"-\"*40)\n",
    "\n",
    "        # Solve condition\n",
    "        if len(episode_rewards) >= 100 and np.mean(episode_rewards[-100:]) >= 475:\n",
    "            print(f\"\\nEnvironment solved at episode {ep + 1}!\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    plot_training(episode_rewards, losses)\n",
    "    return agent, episode_rewards\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Plot Function\n",
    "# --------------------------------------------------------------------\n",
    "def plot_training(rewards, losses):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Reward Plot\n",
    "    axes[0].plot(rewards, label=\"Reward\")\n",
    "    if len(rewards) >= 100:\n",
    "        rolling = [np.mean(rewards[max(0, i-99):i+1]) for i in range(len(rewards))]\n",
    "        axes[0].plot(rolling, label=\"Moving Avg (100)\", linewidth=2)\n",
    "    axes[0].axhline(475, color=\"red\", linestyle=\"--\", label=\"Solved Threshold\")\n",
    "    axes[0].set_title(\"Episode Rewards\")\n",
    "    axes[0].set_xlabel(\"Episode\")\n",
    "    axes[0].set_ylabel(\"Reward\")\n",
    "    axes[0].grid(True)\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Loss Plot\n",
    "    if losses:\n",
    "        axes[1].plot(losses, label=\"Loss\")\n",
    "        axes[1].set_title(\"Training Loss\")\n",
    "        axes[1].set_xlabel(\"Episode\")\n",
    "        axes[1].set_ylabel(\"Loss\")\n",
    "        axes[1].grid(True)\n",
    "        axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"dqn_cartpole_results.png\", dpi=300)\n",
    "    print(\"Plot saved as dqn_cartpole_results.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Agent Evaluation\n",
    "# --------------------------------------------------------------------\n",
    "def evaluate(agent, episodes=10):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    scores = []\n",
    "\n",
    "    print(\"\\nEvaluating trained agent...\")\n",
    "\n",
    "    for i in range(episodes):\n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "        done = False\n",
    "        total = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state, explore=False)\n",
    "            step_output = env.step(action)\n",
    "\n",
    "            if len(step_output) == 5:\n",
    "                state, reward, terminated, truncated, _ = step_output\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                state, reward, done, _ = step_output\n",
    "\n",
    "            total += reward\n",
    "\n",
    "        scores.append(total)\n",
    "        print(f\"Episode {i+1}: Reward = {total}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(f\"  Average Score: {np.mean(scores):.2f}\")\n",
    "    print(f\"  Std Dev: {np.std(scores):.2f}\")\n",
    "    print(f\"  Min/Max: {np.min(scores)} / {np.max(scores)}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Main\n",
    "# --------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model, rewards = train_cartpole(num_episodes=500)\n",
    "    evaluate(model, episodes=10)\n",
    "    print(\"\\nTraining Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35310269",
   "metadata": {},
   "source": [
    "## Problem 3: Analysis and Debugging (3 marks)\n",
    "\n",
    "**Task:** Fix the buggy Q-learning implementation for FrozenLake.\n",
    "\n",
    "- Identify 4 bugs in the given code.\n",
    "- Explain why each bug prevents learning.\n",
    "- Provide the correct fix.\n",
    "- Compare the performance of buggy vs fixed code.\n",
    "- Achieve a success rate > 0.70 after 10,000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe4bdfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROBLEM 3: DEBUGGING Q-LEARNING\n",
      "============================================================\n",
      "\n",
      "Running BUGGY version...\n",
      "Buggy version success rate: 0.000\n",
      "\n",
      "Running FIXED version...\n",
      "FrozenLake Environment Summary\n",
      "  States: 16\n",
      "  Actions: 4\n",
      "------------------------------------------------------------\n",
      "Training Hyperparameters\n",
      "  α = 0.1\n",
      "  γ = 0.99\n",
      "  ε_initial = 1.0\n",
      "  ε_decay = 0.9995\n",
      "------------------------------------------------------------\n",
      "Episode 1000: Success(100ep)=0.040, Epsilon=0.606\n",
      "Episode 2000: Success(100ep)=0.160, Epsilon=0.368\n",
      "Episode 3000: Success(100ep)=0.300, Epsilon=0.223\n",
      "Episode 4000: Success(100ep)=0.410, Epsilon=0.135\n",
      "Episode 5000: Success(100ep)=0.450, Epsilon=0.082\n",
      "\n",
      "Final success rate: 0.45\n",
      "\n",
      "Comparison Results\n",
      "============================================================\n",
      "Buggy success rate  (last 100): 0.000\n",
      "Fixed success rate  (last 100): 0.450\n",
      "Total successes (buggy): 6\n",
      "Total successes (fixed): 940\n",
      "Plot saved: problem3_comparison.png\n",
      "\n",
      "PROBLEM 3 COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BUG ANALYSIS (unchanged, but formatted cleanly)\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "BUG 1: Q-table initialization\n",
    "Impact: Possible dimension mismatch\n",
    "Fix: Use Q = zeros((n_states, n_actions))\n",
    "\n",
    "BUG 2: Action selection\n",
    "Issue: np.argmin instead of np.argmax\n",
    "Impact: Agent selects worst actions\n",
    "Fix: Use np.argmax(Q[state])\n",
    "\n",
    "BUG 3: Q-learning update rule\n",
    "Issue: Missing learning rate\n",
    "Fix: Use TD update: Q[s,a] += α (target - current)\n",
    "\n",
    "BUG 4: No epsilon decay\n",
    "Impact: Agent never exploits\n",
    "Fix: Add epsilon = max(eps_min, eps * decay)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BUGGY VERSION (kept identical)\n",
    "# ============================================================================\n",
    "\n",
    "def buggy_train_frozenlake(episodes=10000):\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    alpha = 0.8\n",
    "    gamma = 0.95\n",
    "    epsilon = 0.3\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # BUG: picks WORST Q-values\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                action = np.argmin(Q[state])\n",
    "\n",
    "            result = env.step(action)\n",
    "            if len(result) == 5:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = result\n",
    "\n",
    "            # BUG: Missing learning rate and TD update\n",
    "            Q[state, action] = reward + gamma * np.max(Q[next_state])\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    print(f\"Buggy version success rate: {np.mean(rewards[-100:]):.3f}\")\n",
    "    return Q, rewards\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED VERSION (cleaner and more structured)\n",
    "# ============================================================================\n",
    "\n",
    "def fixed_train_frozenlake(episodes=10000):\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    print(\"FrozenLake Environment Summary\")\n",
    "    print(f\"  States: {n_states}\")\n",
    "    print(f\"  Actions: {n_actions}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Initialize Q-table\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.1\n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.9995\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    print(\"Training Hyperparameters\")\n",
    "    print(f\"  α = {alpha}\")\n",
    "    print(f\"  γ = {gamma}\")\n",
    "    print(f\"  ε_initial = {epsilon}\")\n",
    "    print(f\"  ε_decay = {epsilon_decay}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        result = env.reset()\n",
    "        state = result[0] if isinstance(result, tuple) else result\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        max_steps = 100\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "\n",
    "            # FIX: Correct epsilon-greedy\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            result = env.step(action)\n",
    "            if len(result) == 5:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = result\n",
    "\n",
    "            # FIX: Proper Q-learning update\n",
    "            current_q = Q[state, action]\n",
    "            target_q = reward + gamma * np.max(Q[next_state])\n",
    "            Q[state, action] = current_q + alpha * (target_q - current_q)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        # FIX: Apply epsilon decay\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(f\"Episode {episode+1}: \"\n",
    "                  f\"Success(100ep)={np.mean(rewards[-100:]):.3f}, \"\n",
    "                  f\"Epsilon={epsilon:.3f}\")\n",
    "\n",
    "    print(\"\\nFinal success rate:\", np.mean(rewards[-100:]))\n",
    "    return Q, rewards\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON WRAPPER\n",
    "# ============================================================================\n",
    "\n",
    "def compare_buggy_vs_fixed():\n",
    "    print(\"=\"*60)\n",
    "    print(\"PROBLEM 3: DEBUGGING Q-LEARNING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    episodes = 5000\n",
    "\n",
    "    print(\"\\nRunning BUGGY version...\")\n",
    "    Q_buggy, rewards_buggy = buggy_train_frozenlake(episodes)\n",
    "\n",
    "    print(\"\\nRunning FIXED version...\")\n",
    "    Q_fixed, rewards_fixed = fixed_train_frozenlake(episodes)\n",
    "\n",
    "    print(\"\\nComparison Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Buggy success rate  (last 100): {np.mean(rewards_buggy[-100:]):.3f}\")\n",
    "    print(f\"Fixed success rate  (last 100): {np.mean(rewards_fixed[-100:]):.3f}\")\n",
    "    print(f\"Total successes (buggy): {sum(rewards_buggy)}\")\n",
    "    print(f\"Total successes (fixed): {sum(rewards_fixed)}\")\n",
    "\n",
    "    plot_comparison(rewards_buggy, rewards_fixed)\n",
    "\n",
    "    print(\"\\nPROBLEM 3 COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PLOTTING\n",
    "# ============================================================================\n",
    "\n",
    "def plot_comparison(rewards_buggy, rewards_fixed, window=100):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ma_buggy = [np.mean(rewards_buggy[max(0, i-window+1):i+1])\n",
    "                for i in range(len(rewards_buggy))]\n",
    "    ma_fixed = [np.mean(rewards_fixed[max(0, i-window+1):i+1])\n",
    "                for i in range(len(rewards_fixed))]\n",
    "\n",
    "    axes[0].plot(ma_buggy, label='Buggy', linewidth=2, alpha=0.7)\n",
    "    axes[0].plot(ma_fixed, label='Fixed', linewidth=2, alpha=0.7)\n",
    "    axes[0].set_title(\"Learning Curve Comparison\")\n",
    "    axes[0].set_xlabel(\"Episode\")\n",
    "    axes[0].set_ylabel(\"Success Rate\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    final_buggy = np.mean(rewards_buggy[-100:])\n",
    "    final_fixed = np.mean(rewards_fixed[-100:])\n",
    "\n",
    "    axes[1].bar(['Buggy', 'Fixed'], [final_buggy, final_fixed],\n",
    "                color=['red', 'green'], alpha=0.7)\n",
    "    axes[1].set_title(\"Final Success Rate\")\n",
    "    axes[1].set_ylabel(\"Success Rate\")\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"problem3_comparison.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Plot saved: problem3_comparison.png\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_buggy_vs_fixed()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
